[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Remote Sensing & Earth Observation Studies",
    "section": "",
    "text": "About Me\nMy name is Nikhil Desai, I am originally from British Columbia, Canada. I completed my undergraduate MA in Economics from the University of Edinburgh, then worked as a management consultant for 3 years. I left this role and am now pursing an MSc in Urban Spatial Science at The Centre for Advanced Spatial Analysis. I’ve been lucky to have lived in a variety of cities in Canada, The Netherlands, New Zealand and now the UK. I think this has caused me to pay attention to the built environment and how it shapes our lives and changes over time.\n\n\n\nOn this site, you will find my week-by-week reflections on my Remote Sensing & Earth Observation course. I am very interested in the intersection of climate change and remote sensing, particularly in relation to deforestation and wildfires. Since 2017, wildfires in British Columbia have been the worst in the province’s history, burning over 1.2 million hectares of land. The 2018 fires were so severe that the smoke from the fires was visible from space as seen below.\n\n\n\nI’m looking forward to look at some ideas and concepts I am passionate about with a new lens.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "2  Week 1",
    "section": "",
    "text": "3 Introduction to Remote Sensing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week1.html#active-and-passive-sensors",
    "href": "Week1.html#active-and-passive-sensors",
    "title": "2  Week 1",
    "section": "3.1 Active and Passive Sensors",
    "text": "3.1 Active and Passive Sensors\nThere are two primary ways to gather remote sensing data: Active and Passive sensors.\n\nActive sensors emit energy in the form of radiation, then detect and measure the radiation that is back scattered or reflected back up to the satellite. An example is SAR, which emits signals towards the Earth’s surface and records the reflected signals to form images (Kreucher, Kastella, and Hero 2005).\nPassive sensors measure natural energy that is reflected or emitted from the Earth’s surface or atmosphere, without actively emitting any signal of their own (Liu 2013). An example is the Landsat programme, which captures the Earth’s surface in the visible, near-infrared, and thermal infrared wavelengths.This distinction is illustrated below:\n\n\n\n\n\n\nPassive versus Active Sensors\n\n\n\n\nSource: Geospatial World",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week1.html#how-remote-sensors-gather-and-store-data",
    "href": "Week1.html#how-remote-sensors-gather-and-store-data",
    "title": "2  Week 1",
    "section": "3.2 How Remote Sensors Gather and Store Data",
    "text": "3.2 How Remote Sensors Gather and Store Data\nRemote sensing technology captures data about Earth’s surface and atmosphere by detecting electromagnetic signatures, which are distinct patterns of energy radiated or reflected by materials and phenomena.\nData from remote sensing are stored with attention to different types of resolution - spatial, spectral, temporal, and radiometric - each providing unique insights into the Earth’s physical characteristics and changes over time.\nSpatial resolution defines the smallest object that can be observed on the ground, determining the detail visible in an image.\nSpectral resolution refers to a sensor’s ability to distinguish between different wavelengths of light, enabling identification of various materials based on their electromagnetic signatures.\nTemporal resolution measures how often a sensor can capture images of the same area over time, crucial for tracking changes and trends.\nRadiometric resolution indicates the sensor’s sensitivity to different levels of light intensity, allowing for the detection of subtle differences in reflectance or emission.\nSource: Verde, 2018",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week1.html#using-sentinel-application-platform-snap",
    "href": "Week1.html#using-sentinel-application-platform-snap",
    "title": "2  Week 1",
    "section": "3.3 Using Sentinel Application Platform (SNAP)",
    "text": "3.3 Using Sentinel Application Platform (SNAP)\nSNAP is a common tool used to process and analyse remote sensing data. It is particularly useful for working with data from the Sentinel satellites. There are a variety of statistics you can generate using SNAP. I’d never thought that you could generate so much data from pictures. It has changed the way in which I think about imagery and the analysis that can be conducted on it.\n\n3.3.1 Analysing data on SNAP is done in the following steps:\n\nDownloading Data Go to the Copernicus website and search for satellite images. You can pick what kind of images you want by setting things like the date or place you’re interested in. Once you find the right images, you download them to your computer.\nLoading Data Open the SNAP program on your computer. There’s an option to open or import files - use it to find and select the satellite images you downloaded. This loads them into SNAP for you to work with.\nAnalysing Images With your images loaded in SNAP, you can now start exploring them. SNAP has tools that let you adjust how the images look, highlight different features, or even combine different images to get a better understanding of what you’re seeing. You can play around with these tools to discover more about the area you’re studying.\n\nHowever after spending a significant portion of my day troubleshooting SNAP - It seems it does not work on iOS Ventura. QGIS to the rescue!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week1.html#references",
    "href": "Week1.html#references",
    "title": "2  Week 1",
    "section": "3.4 References",
    "text": "3.4 References\n\n\n\n\nBraun, A., F. Falah, and V. Hochschild. 2019. “Refugee Camp Monitoring and Environmental Change Assessment of Kutupalong, Bangladesh, Based on Radar Imagery of Sentinel-1 and ALOS-2.” Remote. Sens. 11: 2047. https://doi.org/10.3390/rs11172047.\n\n\nClaverie, M., J. Ju, J. Masek, J. Dungan, E. Vermote, J. Roger, S. Skakun, and C. Justice. 2018. “The Harmonized Landsat and Sentinel-2 Surface Reflectance Data Set.” Remote Sensing of Environment. https://doi.org/10.1016/J.RSE.2018.09.002.\n\n\nCracknell, A. 2018. “The Development of Remote Sensing in the Last 40 Years.” International Journal of Remote Sensing 39: 8387–427. https://doi.org/10.1080/01431161.2018.1550919.\n\n\nKreucher, C., K. Kastella, and A. Hero. 2005. “Sensor Management Using an Active Sensing Approach.” Signal Process. 85: 607–24. https://doi.org/10.1016/j.sigpro.2004.11.004.\n\n\nLiu, J. G. 2013. “REMOTE SENSING | Passive Sensors,” 431–39. https://doi.org/10.1016/B978-0-12-409548-9.02956-0.\n\n\nNASA. 2024. “What Is Synthetic Aperture Radar?” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nNiroumand-Jadidi, M., and F. Bovolo. 2021. “Sentinel-2 Reveals Abrupt Increment of Total Suspended Matter While Ever Given Ship Blocked the Suez Canal.” Water. https://doi.org/10.3390/w13223286.\n\n\nOlsen, R. 2007. “Introduction to Remote Sensing.” BioScience 27: 1–31. https://doi.org/10.1117/3.673407.CH1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week2.html",
    "href": "Week2.html",
    "title": "3  Week 2",
    "section": "",
    "text": "A short presentation on one of the most important space programmes in the world",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "4  Week 3",
    "section": "",
    "text": "4.1 Corrections and Enhancements",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week3.html#corrections-and-enhancements",
    "href": "Week3.html#corrections-and-enhancements",
    "title": "4  Week 3",
    "section": "",
    "text": "4.1.1 Summary\nThis week we discussed how to transform raw data into a product we can use for analysis. This is know as enhancement or correction.\n“Correction” describes the process of removing or reducing the effects of various factors distort the imagery or data. These factors can include atmospheric effects (CLOUDS!) and sensor noise (Bannari et al. 1995).\nThe three main correction types are detailed below, along with their common solutions:\n\n4.1.1.1 1. Geometric & Topographic Error\nGeometric and topographic errors are mistakes in how objects and landforms look in images, caused by camera angles, the shape of the Earth and uneven ground, which need to be fixed to get accurate pictures.\nA common method for geometric correction is orthorectification. This fixes distortions in aerial photos by aligning them with ground truth, correcting for camera angles and terrain.\nThe image below outlines how orthorectification works:\n\n\n\n\n\n\n\n\n\nSource: Abdul Basith\nThis method essentially ensures every point on a photo maps to the right spot on Earth, ensuring accurate measurements and analysis. This step is key for reliable map-making and data overlay from satellite imagery [Jia et al. (2013)](Dave, Joshi, and Srivastava 2015).\n\n\n4.1.1.2 2. Atmospheric Distortion\nAtmospheric distortion occurs when gases and particles in the air change how light travels from the Earth to the camera, making images look blurry or colored differently. This can mislead observations.\nAtmospheric Distortion is frequently corrected using a method called Dark Object Subtraction (DOS). DOS corrects atmospheric distortion by adjusting for the darkest parts. It assumes these should be completely dark or given a value of 0 - it then adjusts the rest of the image using this as a reference (Song et al. 2001).\nDOS removes the effects of atmospheric gases and particles on the recorded signal to accurately reflect the Earth’s surface reflectance. This is essentially wiping away the haze from satellite imagery. (Song et al. 2001). I’ve detailed this method below\nBelow is an image of China’s Three Gorges Area before and after atmospheric correction.\n\n\n\n\n\nThree Gorges Area, China - Before and After Correction\n\n\n\n\nSource: Zhaohua Chen\n\n\n4.1.1.3 3. Radiometric Calibration\nAdjusts the digital image data to correct for sensor noise and inconsistencies, ensuring uniform brightness and contrast across the image.\nWhile, I wouldn’t consider this to an error - you can correct for radiometric variations by applying a calibration equation that converts Digital Number (DN) values to spectral radiance (using gain and bias parameters). This helps to standardise image brightness across datasets.\n\n\n\n4.1.2 Data Joining\nIn Earth observation, data joining involves combining multiple satellite images to create a single image. This is an extremely useful method, as most large areas cannot be captured in a single satellite pass.\nOne common technique for data joining is called mosaic feathering. Seen below:\n\n\n\n\n\n\n\n\n\nSource: Yun Gao\nSimilar to feathering in panoramic photography, mosaic feathering in satellite imagery involves overlapping adjacent images and blending their edges. This blending ensures that the transition between images is smooth, without visible seams or abrupt changes in color and texture that can occur due to differences in lighting, angle of capture, or atmospheric conditions at the time each image was taken.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week3.html#data-joining",
    "href": "Week3.html#data-joining",
    "title": "4  Week 3",
    "section": "4.2 2. Data Joining",
    "text": "4.2 2. Data Joining\nIn Earth observation, data joining involves combining multiple satellite images to create a single image. This is an extremely useful method, as most large areas cannot be captured in a single satellite pass.\nOne common technique for data joining is called mosaic feathering. Seen below:\n\n\n\n\n\n\n\n\n\nSource: Yun Gao\nSimilar to feathering in panoramic photography, mosaic feathering in satellite imagery involves overlapping adjacent images and blending their edges. This blending ensures that the transition between images is smooth, without visible seams or abrupt changes in color and texture that can occur due to differences in lighting, angle of capture, or atmospheric conditions at the time each image was taken.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week3.html#applications-atmospheric-correction-using-dark-object-subtraction",
    "href": "Week3.html#applications-atmospheric-correction-using-dark-object-subtraction",
    "title": "4  Week 3",
    "section": "4.3 Applications: Atmospheric Correction using Dark Object Subtraction",
    "text": "4.3 Applications: Atmospheric Correction using Dark Object Subtraction\nJensen’s description of Atmospheric Correction in Introductory digital image processing: a remote sensing perspective (2015), provides a good overview of the concept of Dark Object Subtraction (DOS). The idea is that the darkest spot shouldn’t really have any brightness and should have a value of zero, so if it does appear slightly bright, that is likely due to atmospheric haze (Wang et al. 2019), (Hadjimitsis, Clayton, and Retalis 2004).\nI’ve chosen an image of my favourite park near my hometown, Kamloops, British Columbia, Canada to conduct DOS on.\nThe formula for doing such is:\n\n\n\n\n\nDark Object Subtraction Equation\n\n\n\n\nI started with an analysis of the Surface Reflectance. The first image below, is the raw Landsat image. The second image is the same image after applying the DOS method.\n\n\n\n\n\nLandsat image prior to and after Dark Object Subtraction\n\n\n\n\nAfter applying the DOS method, this is what we are left with. While the differences are subtle, the image is noticeably clearer. The first image is more green, there is far less variation in colour and the mountainous region on the left hand side of the image is more distinguished making it easier to identify features.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week3.html#reflections",
    "href": "Week3.html#reflections",
    "title": "4  Week 3",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nPrior to this course, I had little exposure to remotely sensed data. Personally, going through the tedious process of enhancing raw remotely sensed data highlighted the complexity of tools like Google Earth. I will never again take for granted the ability to zoom in on a location and casually scroll around the globe.\nIn the practical, I focused on dealing with atmospheric correction using DOS. I chose this simply because it was the method I technically understood, the least. It required a deep dive into the complexities of how light interacts with the Earth’s atmosphere and affects satellite imagery. After taking a look at the two images above, I initially saw little difference, but when looking again, the distinction between the two is quite evident (as detailed above).\n\n\n\n\nBannari, A., D. Morin, G. Bénié, and F. Bonn. 1995. “A Theoretical Review of Different Mathematical Models of Geometric Corrections Applied to Remote Sensing Images” 13: 27–47. https://doi.org/10.1080/02757259509532295.\n\n\nDave, Chintan, R. Joshi, and S. Srivastava. 2015. “A Survey on Geometric Correction of Satellite Imagery.” International Journal of Computer Applications 116: 24–27. https://doi.org/10.5120/20389-2655.\n\n\nHadjimitsis, D., C. Clayton, and A. Retalis. 2004. “On the Darkest Pixel Atmospheric Correction Algorithm: A Revised Procedure Applied over Satellite Remotely Sensed Images Intended for Environmental Applications” 5239. https://doi.org/10.1117/12.511520.\n\n\nJia, Z., Xiaoping Lu, Wenqian Zang, and Q. Liu. 2013. “Research on Fast Geometric Precision Correction of HJ-1 Remote Sensing Image.” Applied Mechanics and Materials 427-429: 1481–84. https://doi.org/10.4028/www.scientific.net/AMM.427-429.1481.\n\n\nSong, C., C. Woodcock, K. Seto, M. Lenney, and S. Macomber. 2001. “Classification and Change Detection Using Landsat TM Data: When and How to Correct Atmospheric Effects?” Remote Sensing of Environment 75: 230–44. https://doi.org/10.1016/S0034-4257(00)00169-3.\n\n\nWang, Yu, Xiaoyong Wang, Hongyan He, and Guoliang Tian. 2019. “An Improved Dark Object Subtraction Method for Atmospheric Correction of Remote Sensing Images,” 425–35. https://doi.org/10.1007/978-981-13-9917-6_41.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week3.html#references",
    "href": "Week3.html#references",
    "title": "4  Week 3",
    "section": "4.4 References",
    "text": "4.4 References\n\n\n\n\nBannari, A., D. Morin, G. Bénié, and F. Bonn. 1995. “A Theoretical Review of Different Mathematical Models of Geometric Corrections Applied to Remote Sensing Images” 13: 27–47. https://doi.org/10.1080/02757259509532295.\n\n\nDave, Chintan, R. Joshi, and S. Srivastava. 2015. “A Survey on Geometric Correction of Satellite Imagery.” International Journal of Computer Applications 116: 24–27. https://doi.org/10.5120/20389-2655.\n\n\nHadjimitsis, D., C. Clayton, and A. Retalis. 2004. “On the Darkest Pixel Atmospheric Correction Algorithm: A Revised Procedure Applied over Satellite Remotely Sensed Images Intended for Environmental Applications” 5239. https://doi.org/10.1117/12.511520.\n\n\nJia, Z., Xiaoping Lu, Wenqian Zang, and Q. Liu. 2013. “Research on Fast Geometric Precision Correction of HJ-1 Remote Sensing Image.” Applied Mechanics and Materials 427-429: 1481–84. https://doi.org/10.4028/www.scientific.net/AMM.427-429.1481.\n\n\nSong, C., C. Woodcock, K. Seto, M. Lenney, and S. Macomber. 2001. “Classification and Change Detection Using Landsat TM Data: When and How to Correct Atmospheric Effects?” Remote Sensing of Environment 75: 230–44. https://doi.org/10.1016/S0034-4257(00)00169-3.\n\n\nWang, Yu, Xiaoyong Wang, Hongyan He, and Guoliang Tian. 2019. “An Improved Dark Object Subtraction Method for Atmospheric Correction of Remote Sensing Images,” 425–35. https://doi.org/10.1007/978-981-13-9917-6_41.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week4.html",
    "href": "Week4.html",
    "title": "5  Week 4",
    "section": "",
    "text": "5.1 Overview: Wildfires in Western Canada\nThis week, we have been tasked with finding an urban challenge and proposing how remote sesning could be used to address the challenge. Prior to starting this degree I spent the summer in my hometown, Kamloops. It is a small rural town in western Canada. The climate of the region is known as a semi-arid desert. The land cover is predominantly grasslands and forest. With the acceleration of climate change, western Canada has been experiencing severe heatwaves and droughts. The increasingly dry conditions have led to an exponential increase in wildfires.\nDuring this summer air quality were at hazardous levels due to the smoke from the wildfires. Several communities were evacuated, including Lytton, a small town near Kamloops, was evacuated and destroyed by a wildfire. Several statistics I found:\nSource: Canadian Broadcating Corporation\nKamloops is denoted below as a green dot on the following NASA visualisation of average temperatures in summer 2021.\nBritish Coumbia Temperature 2021\nSource: NASA, 2021\nWildfires are quickly approaching the Kamloops city limits and burned three houses last summer and the entirety of the city’s north shore was evacuated. Below is an eerie photo I took from my apartment last summer.\nKamloops Wildfires 2023",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "Week4.html#overview-wildfires-in-western-canada",
    "href": "Week4.html#overview-wildfires-in-western-canada",
    "title": "5  Week 4",
    "section": "",
    "text": "Insurance Bureau of Canada reported 78 million CAD in damages\nThe fire destroyed 90 percent of the village, killed two people and forced the evacuation of nearby First Nations communities\nThe fire largely leveled Lytton’s Main Street, burning the post office, ambulance station, health centre, RCMP detachment, Lytton Hotel and the Lytton Village Office\nThe Lytton Chinese History Museum was lost, along with 1,600 artifacts, museum archives and library",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "Week4.html#proposal-using-remote-sensing-to-address-wildfires",
    "href": "Week4.html#proposal-using-remote-sensing-to-address-wildfires",
    "title": "5  Week 4",
    "section": "5.2 Proposal: Using Remote Sensing to Address Wildfires",
    "text": "5.2 Proposal: Using Remote Sensing to Address Wildfires\nThe use of remote sensing to monitor and predict wildfires is not a new concept. However, the technology has been underutilised in Canada. The Canadian government has been slow to adopt new technologies and has been relying on traditional methods to monitor and predict wildfires. The traditional methods includes the use of weather stations and aerial photography.\nThese methods are not as effective as remote sensing technology. Remote sensing could provide real-time data on the location, size and intensity of wildfires, machine learning methods could then predict the spread of wildfires and high risk areas.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "Week4.html#approach",
    "href": "Week4.html#approach",
    "title": "5  Week 4",
    "section": "5.3 Approach",
    "text": "5.3 Approach\n\n5.3.1 Wildfire Monitoring in Kamloops: A Remote Sensing Approach\nGiven the uptick in wildfire incidents in BC, driven by drier conditions, there is a need to actively monitor and predict these events. Remote sensing offers a lot of promise here, providing frequent, detailed data that can change the game in terms of understanding environmental triggers for wildfires, catching them early and forecasting their movements (Rashkovetsky et al. 2021).\nBased on the teachings and research I’ve done, I’ve taken a stab at outlining how the use of remotely sensed data can detect and predict wildfires in BC.\n\n\n5.3.2 Heat Maps & Fire Detection\nWe can use Land Surface Temperature data from satellites like Landsat or MODIS to spot unusually hot areas that might indicate a fire or a high-risk zone. VIIRS data is incredibly useful for identifying thermal anomalies which allow us to pinpoint fires almost as soon as they start [ding2023].\n\n\n5.3.3 Soil Moisture & Dryness\nRainfall data from the Global Precipitation Measurement mission gives us information on recent precipitation patterns, affecting soil moisture levels and by extension, wildfire risk. The SMAP satellite can tell us how moist the soil is, and when it’s super dry, the risk of wildfires is high (Chaparro et al. 2016).\n\n\n5.3.4 Vegetation Health\nWhat’s on the ground plays a massive role in how wildfires behave. We can check out the health of vegetation and the types of land cover in the area using data from Landsat or Sentinel-2 satellites. If there is significant dry and unhealthy vegetation, it’s like laying out a welcome mat for wildfires (Chaparro et al. 2016).\n\n\n5.3.5 Trends & Predictions\nBy digging into historical data on wildfires, weather patterns, and how land use has changed over time, we can start spotting trends and potential hotspots for future fires. We can then feed this data into machine learning models to forecast where and when the next big wildfire might hit, giving us a head start on preparations (Chaparro et al. 2016).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "Week4.html#strategy-implementation",
    "href": "Week4.html#strategy-implementation",
    "title": "5  Week 4",
    "section": "5.4 Strategy & Implementation",
    "text": "5.4 Strategy & Implementation\n\nGather and Prep the Data Attain relevant datasets: LST, thermal anomalies, rainfall, soil moisture, vegetation for Kamloops and ready them for analysis\nIdentify High Risk Areas Using the thermal and LST data, we can keep an eye on current and potential wildfire zones\nAssess the Risk We’ll mix soil moisture data with vegetation health and past wildfire info to figure out which areas are most likely to catch fire\nForecasting Fire Leverage machine learning to assess the trend data and output predictions on future wildfires, including where they might start and essentially how they could spread\nDashboard Develop dashboard / interface for local emergency services, fire departments and planners to access and use data/models in real-time",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "Week4.html#reflections",
    "href": "Week4.html#reflections",
    "title": "5  Week 4",
    "section": "5.5 Reflections",
    "text": "5.5 Reflections\nThis approach is based upon being proactive rather than reactive when it comes to wildfires in Western Canada. By harnessing the power of remote sensing, we can significantly improve how we monitor, react to and predict wildfires, ultimately saving on resources. My research into the British wildfires monitoring reiterates the point I made last week. The application of advanced remote sensing techniques (such as those used by (Ding2023?) and Chaparro et al. (2016)) has not yet been adopted for mainstream use. The power of this technology is clear, but the challenge lies in integrating it into current operations and getting the necessary backing from policies and funding.\nAs municipalities in western Canada such as Kamloops look to adapt to the changing climate, it’s crucial that they embrace these advanced monitoring techniques and work closely with the scientific community to customise these solutions to their specific needs and challenges.\n\n\n\n\nChaparro, D., M. Piles, M. Vall-llossera, and Adriano Camps. 2016. “Surface Moisture and Temperature Trends Anticipate Drought Conditions Linked to Wildfire Activity in the Iberian Peninsula.” European Journal of Remote Sensing 49: 955–71. https://doi.org/10.5721/EuJRS20164950.\n\n\nRashkovetsky, D., F. Mauracher, M. Langer, and M. Schmitt. 2021. “Wildfire Detection from Multisensor Satellite Imagery Using Deep Semantic Segmentation.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14: 7001–16. https://doi.org/10.1109/JSTARS.2021.3093625.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "Week4.html#references",
    "href": "Week4.html#references",
    "title": "5  Week 4",
    "section": "5.6 References",
    "text": "5.6 References\n\n\n\n\nChaparro, D., M. Piles, M. Vall-llossera, and Adriano Camps. 2016. “Surface Moisture and Temperature Trends Anticipate Drought Conditions Linked to Wildfire Activity in the Iberian Peninsula.” European Journal of Remote Sensing 49: 955–71. https://doi.org/10.5721/EuJRS20164950.\n\n\nRashkovetsky, D., F. Mauracher, M. Langer, and M. Schmitt. 2021. “Wildfire Detection from Multisensor Satellite Imagery Using Deep Semantic Segmentation.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14: 7001–16. https://doi.org/10.1109/JSTARS.2021.3093625.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "Week6.html",
    "href": "Week6.html",
    "title": "6  Week 6",
    "section": "",
    "text": "7 Google Earth Engine",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "Week6.html#summary",
    "href": "Week6.html#summary",
    "title": "6  Week 6",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nSorry in advance for the length of this entry, Google Earth Engine (GEE) is an amazing tool and conducting analysis on my hometown was a treat\nThis week we learned the basics of GEE - It is an amazing tool. It allows users to sidestep the labourious process of downloading and processing satellite imagery. Instead, users can access and analyse earth observation data directly in the cloud. GEE makes dealing with remote sensing data incredibly simple.\nThis week’s lecture started with a slide which illustrated the exponential uptake of GEE, showing the number of academic publications using GEE since 2015. GEE’s uptake is seen below (Pham-Duc et al. 2023):\n\n\n\n\n\nAcademic Publications Using Google Earth Engine\n\n\n\n\nUnlike traditional tools such as SNAP or QGIS, GEE operates in the cloud, providing users with immediate access to its extensive data archives and powerful computing resources. This means users can run complex analyses and algorithms on massive datasets directly within their web browser, without the need for extensive local storage or high computing power.\nThe key features of GEE:\nData Catalog: Google Earth Engine provides access to a diverse and continuously expanding catalog of satellite imagery datasets\nAnalysis Tools: It offers a set of tools and algorithms for processing and analysing geospatial data (image classification, time-series analysis, object detection, change detection etc.)\nCode Editor: The Earth Engine Code Editor provides an interactive development environment where users can write, test and run geospatial analysis code\nVisualisation: GEE allows users to visualise geospatial data and analysis results in interactive maps. A user can explore and interact with the data layers.\nBelow I’ve used GEE to conduct a few analyses using methods we’ve learned about over the past few weeks. I’ve used Vancouver as a case study here. I thought it would interesting to see how its distinctly north american ‘grid system’ would appear.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "Week6.html#applications",
    "href": "Week6.html#applications",
    "title": "6  Week 6",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nI watched Noah Gorelick’s lecture on machine learning in GEE. As a result I ended up diving into another one of his papers, Google Earth Engine: Planetary-scale geospatial analysis for everyone, published in 2017. Gorelick mentions a plethora of different ways you can leverage GEE for analysis, some of those include:\n\nGlobal Forest Change Monitoring: GEE has been used to characterise global forest extent, loss and gain using decision trees and large collections of Landsat scenes\nWater Resources Management: GEE enables the detailed monitoring of water bodies’ extent, health, and surface water changes globally\nAgricultural Monitoring and Yield Estimation: Leveraging GEE for agricultural monitoring, specifically for estimating crop yields by relating output from crop model simulations\nUrban Expansion and Land Use Change Detection: The platform supports the analysis of urban expansion and land use changes\n\nThe real strength/utility of GEE is in the speed it can analyse data. This means you can look at much bigger areas and do more studies than before. To me, this seems like a big deal for studying the environment because it helps us understand and make decisions faster.\nA tangible example of the strength and utility of GEE is its role in the Global Forest Watch initiative. This platform uses GEE to monitor deforestation around the world in near-real-time. Traditionally, analysing satellite images to detect changes in forest cover over vast areas like the Amazon rainforest would take months.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "Week6.html#reflections",
    "href": "Week6.html#reflections",
    "title": "6  Week 6",
    "section": "7.4 Reflections",
    "text": "7.4 Reflections\nGEE offers a wide range of applications that span from environmental monitoring to urban planning and agricultural management. Its vast repository of satellite imagery and geospatial data allows for real-time environmental monitoring, making it indispensable tool up-to-date geographical information to make informed decisions. For instance, in agriculture, GEE enables the analysis of crop health over vast areas, aiding in the efficient management of resources and ultimately leading to increased productivity and reduced costs.\nFrom a user experience standpoint, GEE is very mature. Its interface is both intuitive and user-friendly. The platform’s ability to handle large datasets and perform complex spatial analyses in the cloud, without the need for powerful local computing resources, is particularly impressive. The main thing I noticed when conducting analysis using it vs SNAP or QGIS is just this. It’s easy, fast and vast. I don’t know enough about remote sensing to assert that GEE enables new types of analysis. However, through readings, it is obvious that GEE enables more breadth and depth of RS & EO analysis.\nGEE is probably the most complex tool I’ve learned to use on this course thus far. Having taken a sneak peak of next week’s content, I’m excited expand my understanding of the tool by using more complex methods.\n\n\n\n\nPham-Duc, Binh, Ho Nguyen, Hien Phan, and Quan Tran-Anh. 2023. “Trends and Applications of Google Earth Engine in Remote Sensing and Earth Science Research: A Bibliometric Analysis Using Scopus Database.” Earth Science Informatics 16 (3): 2355–71.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "Week6.html#references",
    "href": "Week6.html#references",
    "title": "6  Week 6",
    "section": "7.4 References",
    "text": "7.4 References\n\n\n\n\nPham-Duc, Binh, Ho Nguyen, Hien Phan, and Quan Tran-Anh. 2023. “Trends and Applications of Google Earth Engine in Remote Sensing and Earth Science Research: A Bibliometric Analysis Using Scopus Database.” Earth Science Informatics 16 (3): 2355–71.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "Week7.html",
    "href": "Week7.html",
    "title": "7  Week 7",
    "section": "",
    "text": "7.1 Summary\nThis week we studied how to use Machine Learning (ML) to classify Earth Observation data. I found this to be the most interesting and useful topic in this module as it cemented how to think about pixels in satellite imagery. This week’s readings, particularly Introductory digital image processing: a remote sensing perspective (Jensen, J.R., 2015) allude to classification as on of the essential skills in remote sensing.\nAfter completing this week’s practical, I really understood how transformative GEE has been to the field of remote sensing. While the speed of processing is far quicker - the level of integration and functionality built into GEE enables users to conducts more broad and and comprehensive analysis, using techniques that would take significant time.\nA quick definition before diving in: Overfitting occurs when a machine learning model learns the details and noise in the training data to the extent that it performs poorly on new, unseen data.\nWe focused on land cover classification, training classification models, as well as evaluating how accurate these models are in correctly classifying pixels. The four main classification methods were discussed this week:\nSource: Krzywinski & Altman, 2017\nSource: Corporate Finance Institute, 2021\nSource: G. Mountrakis , 2011",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "Week7.html#overview",
    "href": "Week7.html#overview",
    "title": "7  Week 7",
    "section": "8.1 1. Overview",
    "text": "8.1 1. Overview\nThis week we studied how to use Machine Learning (ML) to classify Earth Observation data. I found this to be the most interesting and useful topic in this module as it cemented how to think about pixels in satellite imagery. This week’s readings, particularly Introductory digital image processing: a remote sensing perspective (Jensen, J.R., 2015) describe classification as on of the essential skills in remote sensing.\nAfter completing this week’s practical, I understood how transformative GEE has been to the field of remote sensing. While the speed of processing is far quicker - the level of integration and functionality built into GEE enables users to conducts more broad and and comprehensive analysis.\nWe focused on land cover classification, particularly training classification models, as well as evaluating how accurate the model was in correctly classifying pixels. Four main classification methods were discussed this week:\n\nClassification and Regression Trees (CART): CART is a straightforward method that splits data into smaller groups to make everything more similar within each group. It is ideal for its simplicity. CART can handle both categorical and continuous data. However, it is susceptible to overfitting.\n\n\n\n\n\n\n\n\n\n\nSource: Krzywinski & Altman, 2017\n\nRandomForest: This method uses several decision trees to make better predictions and be more reliable. By randomly selecting features, RandomForest can handle categorical data, mitigating overfitting risks inherent in CART. It’s good at working with both straightforward and more complex data by combining many simple models.\n\n\n\n\n\n\n\n\n\n\nSource: Corporate Finance Institute, 2021\n\nNaiveBayes: This approach uses Bayes’ theorem and treats each feature in the data as independent from the others. It’s surprisingly good and quick at making predictions, especially with data that can be neatly categorised. While it’s really meant for categorical data, it can also work with continuous data if you assume the data fits a certain pattern.\nSVM (Support Vector Machines): SVM looks for the best boundary that separates different categories in the data. It’s particularly good for dealing with complex problems where the data has many dimensions. Although it is computationally expensive, its ability to manage detailed and continuous data makes it powerful for identifying patterns that are hard to spot.\n\n\n\n\n\n\n\n\n\n\nSource: G. Mountrakis , 2011\n\n8.1.0.1 Training and Testing Data:\nWhen developing classification model, you must train the model, which helps the model learn the link between features and their categories. Testing is used to check how well the model predicts new, unseen data, helping to make sure it works well in different situations and doesn’t just repeat what it has seen before. Distinct training and testing data ensure a model can accurately predict new, unseen data, preventing it from merely memorising the data it was trained on and thus reducing the risk of overfitting.\nWe looked at two ways of classifying data in GEE:\n\nUsing a RandomForest Classifier: This straightforward method applies the RandomForest classifier directly to the training data to categorise the image. It’s a solid technique that benefits from RandomForest’s ability to combine multiple models, which helps reduce errors and overfitting.\nPixel-Based Approach: This more detailed method prepares training and testing data for each individual pixel, leading to a more accurate identification of different land types. This allows for better training and testing, improving the model’s accuracy in classifying land cover.\n\nAccuracy Assessment:\nThe pixel-based method proved its worth with a thorough accuracy check. By carefully dividing the data into training and testing sets for each pixel, this approach allowed for a precise evaluation of the model’s success. The results showed this method was particularly effective at handling the complexities of classifying different land types, suggesting it can produce highly dependable and widely applicable outcomes.\nThis dive into classification methods and how to check their accuracy has been both challenging and revealing. The deep understanding of various classifiers and the essential role of thorough testing are key for using GEE in environmental studies. This exploration not only highlights GEE’s strong features but also the detailed work needed to achieve accurate classification results.2. Applications",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "Week7.html#applications",
    "href": "Week7.html#applications",
    "title": "7  Week 7",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nThe applications of classification in Earth Observation are vast. These methods offer a powerful way to categorise and understand the world around us. Below, I have included a couple examples of classification in Earth Observation data that I found interesting.\n\n7.2.1 Land Cover Classification - Gebze, Turkey\nWhile this study is simplistic in comparison to contemporary studies, I think it does an excellent job illustrating how classification works:\n\n\n\n\n\n\n\n\n\nSource: Kavzoglu & Colkesen, 2009\nIn the article, SVMs are utilised for land cover classification using Landsat ETM+ and Terra ASTER images of Gebze, Turkey. The effectiveness of SVMs was analysed. The study found that SVMs outperformed the maximum likelihood classifier in terms of overall accuracy and individual class accuracies, demonstrating the robustness and effectiveness of SVMs.\n\n\n7.2.2 Predicting Snake Bites - Ratnapura, Sri Lanka\nThis is likely one of the most unique articles I have read in my academic career. “…by incorporating detailed datasets on snake species, farmer behaviors and climatic factors, this study examines the spatio-temporal dynamics of snakebite risks”. While the article is mostly about agent-based modelling, there are several ingenious uses of SVM in satellite imagery. The example below is one of the inputs they use for their model. They found that certain species of snakes take refuge in certain species of trees. As such, they used satellite imagery to identify the distribution of different species of trees in their region of study. this helped them to identify high risk zones for certain types of snakes based on tree species.\n\n\n\n\n\n\n\n\n\nSource: Goldstein et al., 2021",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "Week7.html#reflections",
    "href": "Week7.html#reflections",
    "title": "7  Week 7",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThis was the first week I felt like I could conduct some coherent and interesting research in the field of remote sensing. I’ve gotten to grips with Google Earth Engine and have a good understanding of different classification methods. As mentioned in the introduction, classification, seems to be one of the essential skills in remote sensing.\nHowever, this diary entry has been rather positive and I wanted to reflect on some of the issues I encountered in the practical which must be taken into account when conducting any sort of classification analysis.\nCommon issues tend to revolve around:\n\n7.3.1 Training and Model Selection:\n\nBalanced Training Data: The accuracy of classification greatly depends on the quality, representativeness of the training data. Non-representative samples can skew the model’s performance. This has significant implications and reminds me of a film I watched a couple years ago called Coded Bias.\nChoice of Classification Model: each model has strengths and weaknesses and are suited to different types of classification tasks. Using the incorrect classification model can lead to poor performance, including lower accuracy, overfitting or underfitting.\n\n\n\n7.3.2 Validation and Accuracy Assessment:\n\nOverfitting and Generalisation: There’s a risk of overfitting the model to the training data, making it perform poorly on unseen data. Ensuring a proper train-test split and using validation techniques like cross-validation can help assess how well the model generalises Ying, 2019.\n\n\n\n\n\nCastilla, G. 2016. “We Must All Pay More Attention to Rigor in Accuracy Assessment: Additional Comment to \"the Improvement of Land Cover Classification by Thermal Remote Sensing\". Remote Sens, 2015, 7, 8368-8390.” Remote. Sens. 8: 288. https://doi.org/10.3390/rs8040288.\n\n\nGunathilaka, M. D. K. L., and S. Fernando. 2022. “Accuracy Assessment of Unsupervised Land Use and Land Cover Classification Using Remote Sensing and Geographical Information Systems.” International Journal of Environment, Engineering and Education. https://doi.org/10.55151/ijeedu.v4i3.73.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "Week7.html#references",
    "href": "Week7.html#references",
    "title": "7  Week 7",
    "section": "7.4 References",
    "text": "7.4 References\n\n\n\n\nCastilla, G. 2016. “We Must All Pay More Attention to Rigor in Accuracy Assessment: Additional Comment to \"the Improvement of Land Cover Classification by Thermal Remote Sensing\". Remote Sens, 2015, 7, 8368-8390.” Remote. Sens. 8: 288. https://doi.org/10.3390/rs8040288.\n\n\nGunathilaka, M. D. K. L., and S. Fernando. 2022. “Accuracy Assessment of Unsupervised Land Use and Land Cover Classification Using Remote Sensing and Geographical Information Systems.” International Journal of Environment, Engineering and Education. https://doi.org/10.55151/ijeedu.v4i3.73.\n\n\nJensen, John R, and Kalmesh Lulla. 1987. “Introductory Digital Image Processing: A Remote Sensing Perspective.”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "Week8.html",
    "href": "Week8.html",
    "title": "8  Week 8",
    "section": "",
    "text": "8.1 Summary\nThis week we continued to study classification of earth observation data, focussing on the distinction between object-based and pixel-based classification. Secondly, we look at other ways to assess the accuracy of our classification results. First, I will provide an overview, including the strengths weaknesses and common applications of object based image classification and pixel based image classification (Superpixel and Subpixel), then dive into some methods of how to assess the accuracy of our classification results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "Week8.html#accuracy-assessment-of-classification",
    "href": "Week8.html#accuracy-assessment-of-classification",
    "title": "8  Week 8",
    "section": "8.2 Accuracy Assessment of Classification",
    "text": "8.2 Accuracy Assessment of Classification\nAccuracy assessments for classification results evaluate how well the classification method has performed by comparing the classified data with reference data (ground truth or true values).\n\n8.2.1 Key Methods Used\n\nConfusion Matrix (Error Matrix): A powerful tool that cross-tabulates the actual categories (from ground truth) against the classified categories. It provides metrics like overall accuracy, user’s and producer’s accuracy, and Kappa coefficient (Jensen and Lulla 2008).\nProducer’s and User’s Accuracy: These metrics derive from the confusion matrix. Producer’s accuracy measures the probability that a ground-truth pixel is correctly classified, while user’s accuracy shows the probability that a pixel classified into a category actually represents that category on the ground (Jensen and Lulla 2008).\nF-1 Score: The harmonic mean of user’s accuracy and producer’s accuracy for a class. It balances both measures and gives a single value that balances both.\n\n\\[F1 = \\frac{2 \\times \\text{TP}}{2 \\times \\text{TP} + \\text{FP} + \\text{FN}}\\]\n\n\n8.2.2 Importance of Conducting Accuracy Assessments\n\nBenchmarking and Comparison: It allows for the comparison of different classification algorithms or methods, helping to select the most suitable one for a specific application.\nIdentifying Errors: Helps in identifying specific classes or areas where classification errors are more common, guiding improvements in the classification process.\n\n\n\n8.2.3 Potential Pitfalls\n\nInadequate Reference Data: The quality and quantity of ground truth data can significantly affect the accuracy assessment. Insufficient or inaccurate reference data may lead to misleading assessment results.\nSpatial and Temporal Mismatches: Differences in the timing of satellite imagery and the collection of reference data can introduce errors due to changes in the landscape or land use (Gorelick et al. 2017).\nClass Ambiguity: In cases where classes are spectrally similar or the land cover is heterogeneous, distinguishing between classes becomes difficult, potentially leading to lower accuracy.\nOverfitting: Especially in machine learning classifiers, there’s a risk of overfitting the model to the training data, which can result in high accuracy for the training dataset but poor generalisation to new, unseen data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "Week8.html#applications",
    "href": "Week8.html#applications",
    "title": "8  Week 8",
    "section": "8.3 Applications",
    "text": "8.3 Applications\nI’ve mentioned several applications of classifications and sources in my description above. So, in this application section instead of discussing case studies, I was keen to do some brainstorming of how classification could be used to help solve a problem close to me. I did not find any prior research on this topic, so I thought it would be interesting to brainstorm an approach this week.\nI spent several summers tree-planting in western Canada as part of forest regeneration efforts. I was employed by logging companies who, by law, must replant all tress they cut down. I was rather disillusioned by this, as there was no logic to the selection of trees being replanted or any concern about the original biodiversity of the forest. Trees were planted for the sake of it with no intention of restoring the forest to its original state.\n\n\n\n\n\nSubpixel Analysis - European Commission, 2024\n\n\n\n\nHere, I see an opportunity for subpixel analysis to enhance forest management, particularly in assessing reforestation efforts after clear cutting. This would be an exercise in change detection, comparing biodiversity levels before logging occurs in an area and after tree planting efforts. The goal would be to ensure that the replanted trees are not only surviving but also contributing to the restoration of the original biodivrsity of the ecosystem.\nSubpixel analysis would allow for a detailed examination of satellite imagery to detect changes in forest biodiversity and density at a finer resolution than what is visible at the pixel level. This could identify the health of newly planted trees within each pixel, even if they occupy only a part of it, providing a nuanced view of regeneration efforts over time.\nI would first apply spectral unmixing to Landsat data. I believe this could isolate the spectral signatures of different vegetation types. This process involves identifying “endmembers,” which are the pure spectral signatures of specific land cover types, including various tree species indicative of the region’s original biodiversity. By assessing how these signatures mix at pixel level, we can estimate the proportions of each vegetation type at the subpixel level, effectively monitoring the progress of reforestation and the restoration of biodiversity over time.\nI think the challenge lies in validating the results of subpixel analysis. this is where I believe an accuracy assessment would help\nOne approach is to harden the subpixel data by converting the fractional coverage of each vegetation type into discrete classification, where pixels are assigned to the land cover type that occupies the majority of their area. This hardened classification can then be compared against high-resolution aerial or drone imagery, serving as ground truth.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "Week8.html#reflections",
    "href": "Week8.html#reflections",
    "title": "8  Week 8",
    "section": "8.4 Reflections",
    "text": "8.4 Reflections\nThis week’s topic was quite challenging, as it required a deep dive into remote sensing classification techniques and accuracy assessments. It was very interesting to learn some techniques to add conviction and certainty to remote sensing analysis. Over the past weeks we have raced through a wide array of theory and many techniques, it was useful to learn about some techniques which can help us check the accuracy of our results and help to determine whether our choice of classification method was appropriate.\nDoing a bit of brainstorming on how these techniques could be applied to a real-world problem was interesting exercise. In week 4 we were tasked to something similar, however after 4 weeks of learning about methods and technical details, I was able to develop a more specific and detailed idea of how these techniques could be applied to a problem close to me.\nI’ve also been noticing, the way I am writing these diary entries has become far more technical. I think this may be because I have actually learned something. Ha!\n\n\n\n\nCastillejo-González, Isabel Luisa, Francisca López-Granados, Alfonso Garcı́a-Ferrer, José Manuel Peña -Barragán, Montserrat Jurado-Expósito, Manuel Sánchez de la Orden, and Marı́a González-Audicana. 2009. “Object-and Pixel-Based Analysis for Mapping Crops and Their Agro-Environmental Associated Measures Using QuickBird Imagery.” Computers and Electronics in Agriculture 68 (2): 207–15.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment 202: 18–27.\n\n\nJensen, John R, and Kalmesh Lulla. 2008. “Introductory Digital Image Processing: A Remote Sensing Perspective.”",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "Week8.html#references",
    "href": "Week8.html#references",
    "title": "8  Week 8",
    "section": "8.5 References",
    "text": "8.5 References\n\n\n\n\nCastillejo-González, Isabel Luisa, Francisca López-Granados, Alfonso Garcı́a-Ferrer, José Manuel Peña -Barragán, Montserrat Jurado-Expósito, Manuel Sánchez de la Orden, and Marı́a González-Audicana. 2009. “Object-and Pixel-Based Analysis for Mapping Crops and Their Agro-Environmental Associated Measures Using QuickBird Imagery.” Computers and Electronics in Agriculture 68 (2): 207–15.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment 202: 18–27.\n\n\nJensen, John R, and Kalmesh Lulla. 2008. “Introductory Digital Image Processing: A Remote Sensing Perspective.”",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "Week9.html",
    "href": "Week9.html",
    "title": "9  Week 9",
    "section": "",
    "text": "9.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "Week9.html#applications",
    "href": "Week9.html#applications",
    "title": "9  Week 9",
    "section": "9.2 Applications",
    "text": "9.2 Applications\nContinuing with my interest of deforestation and change detection, the paper Monthly mapping of forest harvesting using dense time series Sentinel-1 written by Zhao et al. uses Sentinel-1 SAR imagery and deep learning to map monthly forest harvesting in California, USA, and Rondônia, Brazil. It compares a SAR based approach with an Optical Imagery based approach. Two distinctly different forest types. Interestingly the approach using SAR outperformed traditional object-based methods (Zhao et al. 2022).\nBelow, illustrates classification conducted using SAR imagery vs Optical Imagery.\n\n\n\n\n\nSAR Imagery Classification\n\n\n\n\n\n\n\n\n\nOptical Imagery Classification\n\n\n\n\nForests are often humid environments, emitting significant water vapor which can distort traditional optical imagery. SAR, according to the authors has performed better in this type of analysis due to its ability to penetrate cloud cover and operate effectively regardless of weather conditions or time of day. This capability is crucial for consistent monitoring in areas with frequent cloudiness or limited daylight, ensuring reliable data acquisition for monthly forest harvesting mapping. Optical imagery, while offering high-resolution and color-rich details under clear conditions, is limited by its dependence on weather and daylight, making SAR a more robust choice for the study’s objectives (Zhao et al. 2022).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "Week9.html#reflections",
    "href": "Week9.html#reflections",
    "title": "9  Week 9",
    "section": "9.3 Reflections",
    "text": "9.3 Reflections\nInitially skeptical about SAR’s utility due to its lower resolution and lack of color, I now understand its value. SAR’s ability to penetrate clouds and vegetation enables consistent and frequent monitoring, making it a key tool for change detection where optical imagery is limited, showcasing its value to the precision and reliability of earth observation efforts.\nIn addition, this week was very useful for our group project on flood mapping using SAR in Da Nang, Vietnam. I’ve transitioned from understanding the theory to grasping the practical aspects of why SAR is frequently used for flooding mapping. SAR’s ability to detection is key for identifying flood zones, crucial for accurate monitoring. This week’s practical work has deepened my grasp, highlighting SAR’s specific strengths in flood mapping and merging theory with real-world application.\nAnd that’s a wrap for CASA0023 ! WHAT A COURSE! Thank you Andy & Ollie, super cool stuff\n\n\n\n\nKojima, Ryuji, Yuma Kayano, Toshikazu Samura, and Katsumi Tadamura. 2023. “Development of an Automatic Detection System for Natural Disaster Occurrences with Spaceborne SAR Data.” 2023 International Technical Conference on Circuits/Systems, Computers, and Communications (ITC-CSCC), 1–4. https://doi.org/10.1109/ITC-CSCC58803.2023.10212508.\n\n\nMoreira, A., P. Prats-Iraola, M. Younis, G. Krieger, I. Hajnsek, and K. Papathanassiou. 2013. “A Tutorial on Synthetic Aperture Radar.” IEEE Geoscience and Remote Sensing Magazine 1: 6–43. https://doi.org/10.1109/MGRS.2013.2248301.\n\n\nNASA. 2024. “What Is Synthetic Aperture Radar?” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nZhao, Feng, Rui Sun, Liheng Zhong, Ran Meng, Chengquan Huang, Xiaoxi Zeng, Mengyu Wang, Yaxin Li, and Ziyang Wang. 2022. “Monthly Mapping of Forest Harvesting Using Dense Time Series Sentinel-1 SAR Imagery and Deep Learning.” Remote Sensing of Environment 269: 112822.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "Week9.html#references",
    "href": "Week9.html#references",
    "title": "9  Week 9",
    "section": "9.4 References",
    "text": "9.4 References\n\n\n\n\nKojima, Ryuji, Yuma Kayano, Toshikazu Samura, and Katsumi Tadamura. 2023. “Development of an Automatic Detection System for Natural Disaster Occurrences with Spaceborne SAR Data.” 2023 International Technical Conference on Circuits/Systems, Computers, and Communications (ITC-CSCC), 1–4. https://doi.org/10.1109/ITC-CSCC58803.2023.10212508.\n\n\nMoreira, A., P. Prats-Iraola, M. Younis, G. Krieger, I. Hajnsek, and K. Papathanassiou. 2013. “A Tutorial on Synthetic Aperture Radar.” IEEE Geoscience and Remote Sensing Magazine 1: 6–43. https://doi.org/10.1109/MGRS.2013.2248301.\n\n\nNASA. 2024. “What Is Synthetic Aperture Radar?” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nZhao, Feng, Rui Sun, Liheng Zhong, Ran Meng, Chengquan Huang, Xiaoxi Zeng, Mengyu Wang, Yaxin Li, and Ziyang Wang. 2022. “Monthly Mapping of Forest Harvesting Using Dense Time Series Sentinel-1 SAR Imagery and Deep Learning.” Remote Sensing of Environment 269: 112822.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bannari, A., D. Morin, G. Bénié, and F. Bonn. 1995. “A Theoretical\nReview of Different Mathematical Models of Geometric Corrections Applied\nto Remote Sensing Images” 13: 27–47. https://doi.org/10.1080/02757259509532295.\n\n\nBraun, A., F. Falah, and V. Hochschild. 2019. “Refugee Camp\nMonitoring and Environmental Change Assessment of Kutupalong,\nBangladesh, Based on Radar Imagery of Sentinel-1 and ALOS-2.”\nRemote. Sens. 11: 2047. https://doi.org/10.3390/rs11172047.\n\n\nClaverie, M., J. Ju, J. Masek, J. Dungan, E. Vermote, J. Roger, S.\nSkakun, and C. Justice. 2018. “The Harmonized Landsat and\nSentinel-2 Surface Reflectance Data Set.” Remote Sensing of\nEnvironment. https://doi.org/10.1016/J.RSE.2018.09.002.\n\n\nCracknell, A. 2018. “The Development of Remote Sensing in the Last\n40 Years.” International Journal of Remote Sensing 39:\n8387–427. https://doi.org/10.1080/01431161.2018.1550919.\n\n\nDave, Chintan, R. Joshi, and S. Srivastava. 2015. “A Survey on\nGeometric Correction of Satellite Imagery.” International\nJournal of Computer Applications 116: 24–27. https://doi.org/10.5120/20389-2655.\n\n\nHadjimitsis, D., C. Clayton, and A. Retalis. 2004. “On the Darkest\nPixel Atmospheric Correction Algorithm: A Revised Procedure Applied over\nSatellite Remotely Sensed Images Intended for Environmental\nApplications” 5239. https://doi.org/10.1117/12.511520.\n\n\nJia, Z., Xiaoping Lu, Wenqian Zang, and Q. Liu. 2013. “Research on\nFast Geometric Precision Correction of HJ-1 Remote Sensing\nImage.” Applied Mechanics and Materials 427-429:\n1481–84. https://doi.org/10.4028/www.scientific.net/AMM.427-429.1481.\n\n\nKojima, Ryuji, Yuma Kayano, Toshikazu Samura, and Katsumi Tadamura.\n2023. “Development of an Automatic Detection System for Natural\nDisaster Occurrences with Spaceborne SAR Data.” 2023\nInternational Technical Conference on Circuits/Systems, Computers, and\nCommunications (ITC-CSCC), 1–4. https://doi.org/10.1109/ITC-CSCC58803.2023.10212508.\n\n\nKreucher, C., K. Kastella, and A. Hero. 2005. “Sensor Management\nUsing an Active Sensing Approach.” Signal Process. 85:\n607–24. https://doi.org/10.1016/j.sigpro.2004.11.004.\n\n\nLiu, J. G. 2013. “REMOTE SENSING | Passive Sensors,”\n431–39. https://doi.org/10.1016/B978-0-12-409548-9.02956-0.\n\n\nMoreira, A., P. Prats-Iraola, M. Younis, G. Krieger, I. Hajnsek, and K.\nPapathanassiou. 2013. “A Tutorial on Synthetic Aperture\nRadar.” IEEE Geoscience and Remote Sensing Magazine 1:\n6–43. https://doi.org/10.1109/MGRS.2013.2248301.\n\n\nNASA. 2024. “What Is Synthetic Aperture Radar?” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nNiroumand-Jadidi, M., and F. Bovolo. 2021. “Sentinel-2 Reveals\nAbrupt Increment of Total Suspended Matter While Ever Given Ship Blocked\nthe Suez Canal.” Water. https://doi.org/10.3390/w13223286.\n\n\nOlsen, R. 2007. “Introduction to Remote Sensing.”\nBioScience 27: 1–31. https://doi.org/10.1117/3.673407.CH1.\n\n\nSong, C., C. Woodcock, K. Seto, M. Lenney, and S. Macomber. 2001.\n“Classification and Change Detection Using Landsat TM Data: When\nand How to Correct Atmospheric Effects?” Remote Sensing of\nEnvironment 75: 230–44. https://doi.org/10.1016/S0034-4257(00)00169-3.\n\n\nWang, Yu, Xiaoyong Wang, Hongyan He, and Guoliang Tian. 2019. “An\nImproved Dark Object Subtraction Method for Atmospheric Correction of\nRemote Sensing Images,” 425–35. https://doi.org/10.1007/978-981-13-9917-6_41.\n\n\nZhao, Feng, Rui Sun, Liheng Zhong, Ran Meng, Chengquan Huang, Xiaoxi\nZeng, Mengyu Wang, Yaxin Li, and Ziyang Wang. 2022. “Monthly\nMapping of Forest Harvesting Using Dense Time Series Sentinel-1 SAR\nImagery and Deep Learning.” Remote Sensing of\nEnvironment 269: 112822.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Week1.html#summary",
    "href": "Week1.html#summary",
    "title": "2  Week 1",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nRemote Sensing is a subset of Geographic Information Systems. It is the science of collecting & analysing information about the earth’s surface through the use of sensors mounted on satellites or planes (Olsen 2007).\nThe value of Remote Sensing spans various fields. It provides critical data for environmental monitoring, disaster management and urban development to name a few (Cracknell 2018). This week we learned about basic concepts in remote sensing and conducted some rudimentary analysis in R and SNAP (a QGIS for remote sensing data). Below I’ll provide an overview of the technology, its applications and my reflections on the potential of remote sensing.\nWe were introduced to two satellite programmes this week: Landsat and Sentinel. Sentinel-2 data offers a spatial resolution of 10 meters compared to Landsat’s 30 meters creating more detailed observations. Sentinel-2 satellite orbit every 5 days and offer more frequent updates than Landsat’s 16-day cycle. This makes Sentinel-2 particularly valuable for monitoring fast-changing conditions on the Earth’s surface (Claverie et al. 2018). For example the Dynamic World programme is a near real-time land cover dataset that uses satellites to monitor changes in Earth’s surface.\n\n3.1.1 There are two types of sensors: Active & Passive\n\nActive sensors emit energy in the form of radiation, then detect and measure the radiation that is back scattered or reflected back up to the satellite. An example is SAR (Kreucher, Kastella, and Hero 2005).\nPassive sensors measure natural energy reflected or emitted from Earth, without actively emitting any signal of their own (Liu 2013). An example is the Landsat programme, which captures the Earth’s surface in the visible, near-infrared and thermal infrared wavelengths. This is illustrated below:\n\n\n\n\n\n\nPassive versus Active Sensors\n\n\n\n\nSource: Geospatial World\n\n\n3.1.2 How Remote Sensors Store Data\nRemote sensing technology captures data about Earth’s surface and atmosphere by detecting electromagnetic signatures, which are distinct patterns of energy radiated or reflected (NASA 2024).\nData from remote sensing are stored with attention to different types of resolution. Each provides unique insights into the Earth’s physical characteristics.\n\nSpatial resolution indicates the smallest visible feature in an image. Higher resolution reveals finer details\nSpectral resolution is a sensor’s ability to distinguish light wavelengths, critical for identifying materials\nTemporal resolution measures how often an area is imaged, important for tracking changes over time\nRadiometric resolution reflects a sensor’s sensitivity to light intensity, aiding in detecting subtle variations\n\nSource: Verde, 2018\n\n\n3.1.3 Raster Analysis: Kamloops, BC, Canada Sentinel-2 Imagery\nBefore diving into case studies using remote sensing data, I wanted to showcase some basic analysis you can conduct. I’ve downloaded some satelite data of my Kamloops, British Columbia, Canada. Below is a True Colour Images of the area (built from the Blue, Green and Red Bands)\n\n\n\n\n\nKamloops, British Columbia, Spectral Signature\n\n\n\n\nAfter experimenting in QGIS, I extracted the spectral signature alongside some indices of the area. The spectral signature is a a unique combination of reflectance values across red, green, and blue wavelengths. This enables the identification of specific materials or objects on Earth by their color profile\nBelow is the Spectral Signature and Terrain Ruggedness Index of the imagery:\n\n\n\n\n\nKamloops, British Columbia, Spectral Signature\n\n\n\n\nThe Terrain Ruggedness Index quantifies the topographic variation of an area in Earth observation data, measuring the difference in elevation between adjacent pixels to assess the roughness of the terrain (Valentine 2005).\n\n\n\n\n\nKamloops, British Columbia, Terrain Ruggedness Index\n\n\n\n\nHere, I have built a true-colour composite from the red, green, and blue bands of the Sentinel-2 imagery. True-color composite images depict the Earth’s surface in colors similar to those a human eye would perceive, combining red, green, and blue light wavelengths to create natural-looking photographs from satellite data (Sovdat et al. 2019).\n\n\n\n\n\nKamloops, British Columbia, True Colour Composite",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week1.html#applications",
    "href": "Week1.html#applications",
    "title": "2  Week 1",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nRemote sensing gives us a way to see and understand changes on our planet that we can’t detect from the ground. My initial perspective was that remote sensing data was mainly used for analysis of natural phenomena and climate related. After doing some reading, it as evident the applications of remote sensing are vast. Below, I’ve described two interesting case studies I came across.\n\n3.2.1 Tracking the Growth of Refugee Camps\nSentinel-2 imagery, humanitarian organisations mapped the rapid expansion of Rohingya refugee camps, facilitating efficient aid distribution, infrastructure planning, and risk management. This data was instrumental in adapting emergency responses and optimising resource allocation as the camp situation evolved (Braun, Falah, and Hochschild 2019).\n\n\n3.2.2 Investigating the Suez Canal Blockage by the Ever Given\nSentinel-2 images provided clear views of the ship’s position and the traffic jam of vessels waiting to pass through the canal. These images were crucial for assessing the blockage and its impact on global trade (Niroumand-Jadidi and Bovolo 2021)\n\n\n\n\n\nEverGiven Blocking Suez Canal - Satelite Imagery\n\n\n\n\nSource: Forbes, 2021",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week1.html#reflections",
    "href": "Week1.html#reflections",
    "title": "2  Week 1",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\nThe potential of remote sensing to explore humanitarian issues was very eye opening to me, especially after reading a CASA staff member’s (Ollie Ballinger) contributions to Bellingcat, an investigative news outlet which specialises in disaster and war crimes. This technology’s utility extends beyond monitoring natural events to human-related issues, including human rights abuses, war crimes and informal settlements.\n\n\n\n\n\nInformal Settlements in Mumbai India\n\n\n\n\nSource: IDEAtlas\nThis technology was deeply fascinating to me from an academic and commercial perspective. From an academic perspective, the concept of extracting immense amounts of data from imagery is will make me look at photos very differently. Analysis as simple as extracting spectral signatures can provide a wealth of information about the land cover and terrain of an area. Spectral signatures are like fingerprints for different land cover. This capability is crucial for environmental monitoring, a particular interest of mine.\nAfter reading about the applications of Remote Sensing, it is clear that its potential has not been completely tapped into. The field is still in its early stages only from a commercial perspective. I say this because I primarily found application within academic and research settings. The processing of Satelite Imagery prior to Google Earth Engine, as I found out this week, is slow and tedious. GEE was only release to the public in 2010. This gap between academic research and commercial application presents a vast opportunity for innovation and implementation in various industries.\nThis week has been a great intro to Remote Sensing and has whet my appetite for the coming weeks.\n\n\n\n\nBraun, A., F. Falah, and V. Hochschild. 2019. “Refugee Camp Monitoring and Environmental Change Assessment of Kutupalong, Bangladesh, Based on Radar Imagery of Sentinel-1 and ALOS-2.” Remote. Sens. 11: 2047. https://doi.org/10.3390/rs11172047.\n\n\nClaverie, M., J. Ju, J. Masek, J. Dungan, E. Vermote, J. Roger, S. Skakun, and C. Justice. 2018. “The Harmonized Landsat and Sentinel-2 Surface Reflectance Data Set.” Remote Sensing of Environment. https://doi.org/10.1016/J.RSE.2018.09.002.\n\n\nCracknell, A. 2018. “The Development of Remote Sensing in the Last 40 Years.” International Journal of Remote Sensing 39: 8387–427. https://doi.org/10.1080/01431161.2018.1550919.\n\n\nKreucher, C., K. Kastella, and A. Hero. 2005. “Sensor Management Using an Active Sensing Approach.” Signal Process. 85: 607–24. https://doi.org/10.1016/j.sigpro.2004.11.004.\n\n\nLiu, J. G. 2013. “REMOTE SENSING | Passive Sensors,” 431–39. https://doi.org/10.1016/B978-0-12-409548-9.02956-0.\n\n\nNASA. 2024. “What Is Synthetic Aperture Radar?” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nNiroumand-Jadidi, M., and F. Bovolo. 2021. “Sentinel-2 Reveals Abrupt Increment of Total Suspended Matter While Ever Given Ship Blocked the Suez Canal.” Water. https://doi.org/10.3390/w13223286.\n\n\nOlsen, R. 2007. “Introduction to Remote Sensing.” BioScience 27: 1–31. https://doi.org/10.1117/3.673407.CH1.\n\n\nSovdat, Blaz, M. Kadunc, M. Batič, and G. Milcinski. 2019. “Natural Color Representation of Sentinel-2 Data.” Remote Sensing of Environment. https://doi.org/10.1016/J.RSE.2019.01.036.\n\n\nValentine, P. 2005. “Rugged: Terrain Ruggedness Index for the Stellwagen Bank Mapping Project.” Downloadable Data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "4  Week 3",
    "section": "",
    "text": "4.1.1 Corrections and Enhancements\nOver the course of the week, we discussed how to transform raw, remotely sensed data into a product we can use for analysis. This transformation process is know as enhancing or correcting data.\nIn academic terms, “Correction” describes the process of removing or reducing the effects of various factors distort the imagery or data. These factors can include atmospheric effects and sensor noise among others (Bannari et al. 1995).\nThe three main correction types are detailed below, along with their common solutions:\n\n4.1.1.1 Geometric & Topographic Error\nAdjusts for image distortions due to the Earth’s rotation, sensor tilt, and curvature, ensuring accurate spatial positioning.\nA common method for geometric correction is orthorectification. This fixes distortions in aerial photos by aligning them with ground truth, correcting for camera angles and terrain (Jia et al. 2013). This method essentially ensures every point on a photo maps to the right spot on Earth, ensuring accurate measurements and analysis. This step is key for reliable map-making and data overlay from satellite imagery (Dave, Joshi, and Srivastava 2015).\n\n\n\n\n\n\n\n\n\nSource: Abdul Basith\n\n\n4.1.1.2 Atmospheric Distortion\nRemoves the effects of atmospheric gases and particles on the recorded signal to accurately reflect the Earth’s surface reflectance. This is essentially wiping away the haze from satellite imagery. Atmospheric particles and gases can often blur or change the colors in an image. The goal is to make the image show the Earth’s surface as if the atmosphere wasn’t messing with the view (Song et al. 2001).\nRegression is often used to correct atmospheric errors. It allows you to model the relationship between satellite data with known ground values, then applying this model to adjust the entire image for atmospheric distortions. Below is a Landsat image of the Three Gorges Area, China before and after correction for haze (Song et al. 2001).\n\n\n\n\n\nThree Gorges Area, China - Before and After Correction\n\n\n\n\nSource: Zhaohua Chen\n\n\n4.1.1.3 Radiometric Calibration\nAdjusts the digital image data to correct for sensor noise and inconsistencies, ensuring uniform brightness and contrast across the image.\nWhile, I wouldn’t consider this to an error - you can correct for radiometric variations by applying a calibration equation that converts Digital Number (DN) values to spectral radiance (using gain and bias parameters). This helps to standardise image brightness across datasets.\n\n\n\n4.1.2 Data Joining\nIn Earth observation, data joining involves combining multiple satellite images to create a single image. This is an extremely useful method, as most large areas cannot be captured in a single satellite pass.\nOne common technique for data joining is called mosaic feathering. Seen below:\n\n\n\n\n\n\n\n\n\nSource: Yun Gao\nSimilar to feathering in panoramic photography, mosaic feathering in satellite imagery involves overlapping adjacent images and blending their edges. This blending ensures that the transition between images is smooth, without visible seams or abrupt changes in color and texture that can occur due to differences in lighting, angle of capture, or atmospheric conditions at the time each image was taken.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week3.html#applications",
    "href": "Week3.html#applications",
    "title": "4  Week 3",
    "section": "4.2 Applications",
    "text": "4.2 Applications\n\n4.2.1 Atmospheric Correction using Dark Object Subtraction\nJensen’s description of Atmospheric Correction in Introductory digital image processing: a remote sensing perspective (2015), provides a good overview of the concept of Dark Object Subtraction (DOS). The idea is that the darkest spot shouldn’t really have any brightness and should have a value of zero, so if it does appear slightly bright, that is likely due to atmospheric haze (Wang et al. 2019), (Hadjimitsis, Clayton, and Retalis 2004).\nI’ve chosen an image of my favourite park near my hometown, Kamloops, British Columbia, Canada to conduct DOS on.\nThe formula for doing such is:\n\n\n\n\n\nDark Object Subtraction Equation\n\n\n\n\nI started with an analysis of the Surface Reflectance. The first image below, is the raw Landsat image. The second image is the same image after applying the DOS method.\n\n\n\n\n\nLandsat image prior to and after Dark Object Subtraction\n\n\n\n\nAfter applying the DOS method, this is what we are left with. While the differences are subtle, the image is noticeably clearer. The first image is more green, there is far less variation in colour and the mountainous region on the left hand side of the image is more distinguished making it easier to identify features.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "Week9.html#summary",
    "href": "Week9.html#summary",
    "title": "9  Week 9",
    "section": "",
    "text": "9.1.1 Synthetic Aperture Radar (SAR)\nSAR is a particularly powerful tool for change detection, an interest of mine. Learning about SAR, while the methods are familiar, comes across as a unique type of remote sensing due to it being an active sensors and functioning at a different wavelength.\nSAR is an active remote sensing technology that captures high-resolution images of the Earth’s surface. SAR works independently of sunlight, emitting their own energy measure the reflected energy (NASA 2024). This allows SAR to obtain images during the night or through clouds and vegetation (Moreira et al. 2013). The week 1 image which illustrates active vs. passive sensors is useful here.\nSAR uses wavelengths in the microwave band of the electromagnetic spectrum to collect data, enabling it to penetrate through clouds and interact with different surface materials, thereby providing unique insights regardless of weather conditions or time of day (NASA 2024).\n\n\n\n\n\nSAR Wavelenghts\n\n\n\n\nSource: (NASA 2024)\nThe synthetic aspect of Synthetic Aperture Radar refers to how it simulates a much larger antenna from “…a sequence of acquisitions from a shorter antenna are combined to simulate a much larger antenna, thus providing higher resolution data [NASA2024]”\n\n\n9.1.2 Backscatter and Polarisation\nSAR sensors operate by measuring the reflections or backscatter of these waves. The variation in amplitude and phase of these reflections, due to differences in surface properties or changes such as damage, allows for the detection of changes on the ground, including the aftermath of disasters, independent of light or weather conditions (Kojima et al. 2023).\n\n\n\n\n\nThe concept of backscatter\n\n\n\n\nSource: (NASA 2024)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "7  Week 7",
    "section": "",
    "text": "Classification and Regression Trees (CART) CART is a straightforward method that splits data into smaller groups to make everything more similar within each group. It is ideal for its simplicity. CART can handle both categorical and continuous data. However, it is susceptible to overfitting (jensen1987?).\n\n\n\n\nRandomForest: This method uses several decision trees to make better predictions and be more reliable. By randomly selecting features, RandomForest can handle categorical data, mitigating overfitting risks inherent in CART. It’s good at working with both straightforward and more complex data by combining many simple models (jensen1987?).\n\n\n\n\nNaiveBayes: This approach uses Bayes’ theorem and treats each feature in the data as independent from the others. It’s surprisingly good and quick at making predictions, especially with data that can be neatly categorised. While it’s really meant for categorical data, it can also work with continuous data if you assume the data fits a certain pattern (jensen1987?).\nSVM (Support Vector Machines): SVM looks for the best boundary that separates different categories in the data. It’s particularly good for dealing with complex problems where the data has many dimensions. Although it is computationally expensive, its ability to manage detailed and continuous data makes it powerful for identifying patterns that are hard to spot (jensen1987?).\n\n\n\n\n7.1.1 Training and Testing Data:\nWhen developing classification model, you must train the model, which helps the model learn the link between features and their categories. Testing is then used to check how well the model predicts new, unseen data, helping to make sure it works well in different situations and doesn’t just repeat what it has seen before. Distinct training and testing data ensure a model can accurately predict new, unseen data, preventing it from merely memorising the data it was trained on and thus reducing the risk of overfitting.\nWe looked at two ways of classifying data in GEE:\n\nUsing a RandomForest Classifier: This straightforward method applies the RandomForest classifier directly to the training data to categorise the image. It’s a solid technique that benefits from RandomForest’s ability to combine multiple models, which helps reduce errors and overfitting.\nPixel-Based Approach: This more detailed method prepares training and testing data for each individual pixel, leading to a more accurate identification of different land types. This allows for better training and testing, improving the model’s accuracy in classifying land cover.\n\n\n\n7.1.2 Accuracy Assessment:\nIn remote sensing machine learning, accuracy assessments evaluate how well a model’s predictions match the true observed values, often through metrics like the kappa coefficient and producer’s accuracy. These assessments help in understanding the model’s performance in classifying or predicting outcomes based on satellite imagery or spatial data.\n\nKappa coefficient: A statistical measure that compares an observed accuracy with an expected accuracy (random chance), taking into account the possibility of the agreement occurring by chance (Castilla 2016)\nProducer’s accuracy: The likelihood that a certain class in reality is correctly predicted by the model, indicating the model’s ability to avoid omission errors for that class (Gunathilaka and Fernando 2022)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "8  Week 8",
    "section": "",
    "text": "8.1.1 Pixel-Based Image Analysis\nIn pixel-based image analysis, each pixel is classified independently based on spectral signature. This method includes sub-pixel analysis, which estimates the fractional coverage of different land cover types within a single pixel (Castillejo-González et al. 2009).\n\nStrengths Offers high precision in identifying land cover types within mixed pixels, ideal for fine-scale analysis\nWeaknesses Complexity in selecting appropriate endmembers and challenges in accuracy assessment\nCommon Uses Widely used in agricultural monitoring, where detailed differentiation between crop types within a single pixel is critical. This is illustrated below (Castillejo-González et al. 2009)\n\n\n\n\n\n\nSuperpixel Analysis - Mclachlan et al., 2017\n\n\n\n\nThe second method is superpixel analysis. This method groups similar pixels into larger superpixels for more efficient processing and analysis (Jensen and Lulla 2008) .\n\nStrengths Simplifies the image by reducing the number of individual elements to be analysed, making it easier to manage and classify large datasets\nWeaknesses The choice of parameters for clustering can greatly affect the outcome, and inappropriate settings may lead to over- or under-segmentation (Jensen and Lulla 2008)\nCommon Uses Useful for large-scale land cover mapping where you are delineating homogeneous areas. K-means clustering are aid in creating super-pixels(Castillejo-González et al. 2009)\n\n\n\n\n\n\nSubpixel Analysis - Liu et al., 2022\n\n\n\n\n\n\n8.1.2 Object-Based Image Analysis (OBIA)\nOBIA groups adjacent pixels into objects based on their spectral characteristics. This approach considers the texture of features within an image (Jensen and Lulla 2008).\n\nStrengths: Good at handling spatial patterns and relationships between objects, reducing ‘salt and pepper’ noise\nWeaknesses: computationally intensive and requires parameter tuning for segmenting images into meaningful objects (Gorelick et al. 2017)\nCommon Uses: Primarily used in urban planning, forest mapping, and habitat delineation, where the spatial arrangement and context of objects (like buildings or vegetation patches) are important. Techniques like image gradient and spectral gradient analysis are sub-methods within OBIA.\n\n\n \n\n\n\n\nSource: Kate Alison, 2023 and GIS Geography, 2024",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "Week6.html#applications-1",
    "href": "Week6.html#applications-1",
    "title": "6  Week 6",
    "section": "7.2 Applications 1",
    "text": "7.2 Applications 1\n\n7.2.1 NDVI Analysis\nNDVI Analysis assesses vegetation health and coverage in a given area. By comparing the reflectance of near-infrared and red light, NDVI helps to distinguish between vegetated and non-vegetated surfaces, indicating plant health and biomass. High indexes are seen in Stanley park, the large green area seen at the top of the window and on the left tip of the city is the University Endowment Lands, an old growth protected area where the University of British Columbia is located. Due to housing shortages in the city, several acres of this protected area have sadly been approved for development. I will be checking to see how this area changes over the next few years.\n\n\n\n\n\nNDVI Analysis of Vancouver\n\n\n\n\n\n\n7.2.2 PCA\nIn essence, Principal Component Analysis is used to emphasise variation and bring out patterns in a dataset. In remote sensing, PCA transforms correlated bands in an image into uncorrelated variables or principal components. This method is useful for enhancing the differences of satellite imagery while retaining most of the original information “…often revealing hidden patterns in the data.”\n\n\n\n\n\nPCA Analysis of Vancouver\n\n\n\n\n\n\n7.2.3 Texture Analysis\nFinally Texture Analysis involves evaluating the spatial arrangement of colors or intensities in an image to identify patterns or structures. By applying methods such as GLCM, texture analysis can highlight areas of contrast or uniformity which can provide insights about the physical characteristics and differences of the surface. This technique is useful for things like classifying land cover, detecting change and understanding the structure features in the landscape.\n\n\n\n\n\nTexture Analysis of Vancouver",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "Week6.html#applications-2",
    "href": "Week6.html#applications-2",
    "title": "6  Week 6",
    "section": "7.3 Applications 2",
    "text": "7.3 Applications 2\nI watched Noah Gorelick’s lecture on machine learning in GEE. As a result I ended up diving into another one of his papers, Google Earth Engine: Planetary-scale geospatial analysis for everyone, published in 2017. Gorelick mentions a plethora of different ways you can leverage GEE for analysis, some of those include:\n\nGlobal Forest Change Monitoring: GEE has been used to characterise global forest extent, loss and gain using decision trees and large collections of Landsat scenes\nWater Resources Management: GEE enables the detailed monitoring of water bodies’ extent, health, and surface water changes globally\nAgricultural Monitoring and Yield Estimation: Leveraging GEE for agricultural monitoring, specifically for estimating crop yields by relating output from crop model simulations\nUrban Expansion and Land Use Change Detection: The platform supports the analysis of urban expansion and land use changes\n\nThe real strength/utility of GEE is in the speed it can analyse data. This means you can look at much bigger areas and do more studies than before. To me, this seems like a big deal for studying the environment because it helps us understand and make decisions faster.\nA tangible example of the strength and utility of GEE is its role in the Global Forest Watch initiative. This platform uses GEE to monitor deforestation around the world in near-real-time. Traditionally, analysing satellite images to detect changes in forest cover over vast areas like the Amazon rainforest would take months.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  }
]